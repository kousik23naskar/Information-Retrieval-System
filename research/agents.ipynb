{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "from langchain_community.tools import ArxivQueryRun\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "#from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import create_openai_tools_agent, AgentExecutor\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "#from transformers import pipeline\n",
    "from langchain import hub\n",
    "from IPython.display import HTML, display, Markdown\n",
    "import torch\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_wrapper=WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=200)\n",
    "wiki=WikipediaQueryRun(api_wrapper=api_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kousik/Desktop/Learning_projects/Langchain_tutorial/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9000). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "Number of GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load .env file to environment\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "## Langsmith tracking\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "groq_api_key=os.environ['GROQ_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kousik/Desktop/Learning_projects/Langchain_tutorial/venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "## Embedding Using Huggingface\n",
    "huggingface_embeddings=HuggingFaceBgeEmbeddings(\n",
    "    #model_name=\"BAAI/bge-small-en-v1.5\",  #sentence-transformers/all-MiniLM-l6-v2\n",
    "    model_name=\"all-MiniLM-L6-v2\",    \n",
    "    model_kwargs={'device':device},\n",
    "    encode_kwargs={'normalize_embeddings':True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a test document.\"\n",
    "query_result = huggingface_embeddings.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.array(query_result).shape)\n",
    "#print(np.array(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc_result = huggingface_embeddings.embed_documents([text, \"This is not a test document.\"])\n",
    "#print(np.array(doc_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f8671211f30>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
    "docs=loader.load()\n",
    "documents=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200).split_documents(docs)\n",
    "#vectordb=FAISS.from_documents(documents,OpenAIEmbeddings())\n",
    "vectordb=FAISS.from_documents(documents,huggingface_embeddings)\n",
    "retriever=vectordb.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retriever_tool=create_retriever_tool(retriever,\"langsmith_search\",\n",
    "                      \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'langsmith_search'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Arxiv Tool\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "from langchain_community.tools import ArxivQueryRun\n",
    "\n",
    "arxiv_wrapper=ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "arxiv=ArxivQueryRun(api_wrapper=arxiv_wrapper)\n",
    "arxiv.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "page_content='Scaled Dot-Product Attention\n",
      " Multi-Head Attention\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      "attention layers running in parallel.\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
      "query with the corresponding key.\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
      "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
      "query with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\n",
      "values.\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together\n",
      "into a matrix Q. The keys and values are also packed together into matrices KandV. We compute\n",
      "the matrix of outputs as:\n",
      "Attention( Q, K, V ) = softmax(QKT\n",
      "√dk)V (1)\n",
      "The two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\n",
      "plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n",
      "of1√dk. Additive attention computes the compatibility function using a feed-forward network with\n",
      "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\n",
      "much faster and more space-efficient in practice, since it can be implemented using highly optimized\n",
      "matrix multiplication code.\n",
      "While for small values of dkthe two mechanisms perform similarly, additive attention outperforms\n",
      "dot product attention without scaling for larger values of dk[3]. We suspect that for large values of\n",
      "dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n",
      "extremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\n",
      "3.2.2 Multi-Head Attention\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\n",
      "we found it beneficial to linearly project the queries, keys and values htimes with different, learned\n",
      "linear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\n",
      "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n",
      "4To illustrate why the dot products get large, assume that the components of qandkare independent random\n",
      "variables with mean 0and variance 1. Then their dot product, q·k=Pdk\n",
      "i=1qiki, has mean 0and variance dk.\n",
      "4' metadata={'source': '../huggingface/papers/1706.03762v7.pdf', 'page': 3}\n"
     ]
    }
   ],
   "source": [
    "pdfloader=PyPDFLoader(\"../huggingface/papers/1706.03762v7.pdf\")\n",
    "\n",
    "pdfdocs=pdfloader.load()\n",
    "print(len(pdfdocs))\n",
    "pdfdocuments=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200).split_documents(pdfdocs)\n",
    "print(pdfdocs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190\n",
      "page_content='Figure 1.2: Larger models make increasingly efﬁcient use of in-context information. We show in-context learning\n",
      "performance on a simple task requiring the model to remove random symbols from a word, both with and without a\n",
      "natural language task description (see Sec. 3.9.2). The steeper “in-context learning curves” for large models demonstrate\n",
      "improved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range\n",
      "of tasks.\n",
      "sufﬁcient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing\n",
      "to a conceptual limitation in our current NLP techniques, this adaptability has practical advantages – it allows humans\n",
      "to seamlessly mix together or switch between many tasks and skills, for example performing addition during a lengthy\n",
      "dialogue. To be broadly useful, we would someday like our NLP systems to have this same ﬂuidity and generality.\n",
      "One potential route towards addressing these issues is meta-learning1– which in the context of language models means\n",
      "the model develops a broad set of skills and pattern recognition abilities at training time, and then uses those abilities\n",
      "at inference time to rapidly adapt to or recognize the desired task (illustrated in Figure 1.1). Recent work [ RWC+19]\n",
      "attempts to do this via what we call “in-context learning”, using the text input of a pretrained language model as a form\n",
      "of task speciﬁcation: the model is conditioned on a natural language instruction and/or a few demonstrations of the task\n",
      "and is then expected to complete further instances of the task simply by predicting what comes next.\n",
      "While it has shown some initial promise, this approach still achieves results far inferior to ﬁne-tuning – for example\n",
      "[RWC+19] achieves only 4% on Natural Questions, and even its 55 F1 CoQa result is now more than 35 points behind\n",
      "the state of the art. Meta-learning clearly requires substantial improvement in order to be viable as a practical method of\n",
      "solving language tasks.\n",
      "Another recent trend in language modeling may offer a way forward. In recent years the capacity of transformer\n",
      "language models has increased substantially, from 100 million parameters [ RNSS18 ], to 300 million parameters\n",
      "[DCLT18 ], to 1.5 billion parameters [ RWC+19], to 8 billion parameters [ SPP+19], 11 billion parameters [ RSR+19],\n",
      "and ﬁnally 17 billion parameters [ Tur20 ]. Each increase has brought improvements in text synthesis and/or downstream\n",
      "NLP tasks, and there is evidence suggesting that log loss, which correlates well with many downstream tasks, follows a\n",
      "smooth trend of improvement with scale [ KMH+20]. Since in-context learning involves absorbing many skills and\n",
      "tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong\n",
      "gains with scale.\n",
      "1In the context of language models this has sometimes been called “zero-shot transfer”, but this term is potentially ambiguous:\n",
      "the method is “zero-shot” in the sense that no gradient updates are performed, but it often involves providing inference-time\n",
      "demonstrations to the model, so is not truly learning from zero examples. To avoid this confusion, we use the term “meta-learning”\n",
      "to capture the inner-loop / outer-loop structure of the general method, and the term “in context-learning” to refer to the inner\n",
      "loop of meta-learning. We further specialize the description to “zero-shot”, “one-shot”, or “few-shot” depending on how many\n",
      "demonstrations are provided at inference time. These terms are intended to remain agnostic on the question of whether the model\n",
      "learns new tasks from scratch at inference time or simply recognizes patterns seen during training – this is an important issue which\n",
      "we discuss later in the paper, but “meta-learning” is intended to encompass both possibilities, and simply describes the inner-outer\n",
      "loop structure.\n",
      "4' metadata={'source': '../huggingface/papers/2005.14165v4.pdf', 'page': 3}\n"
     ]
    }
   ],
   "source": [
    "##custom pdf tool\n",
    "\n",
    "## Read the ppdfs from the folder\n",
    "pdf_loader=PyPDFDirectoryLoader(\"../huggingface/papers\")\n",
    "\n",
    "pdf_docs=pdf_loader.load()\n",
    "print(len(pdf_docs))\n",
    "pdf_documents=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200).split_documents(pdf_docs)\n",
    "print(pdf_docs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f8677bdd540>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## VectorStore Creation\n",
    "pdf_vectorstore=FAISS.from_documents(pdf_documents,huggingface_embeddings)\n",
    "pdf_retriever=pdf_vectorstore.as_retriever()\n",
    "pdf_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_retriever_tool=create_retriever_tool(pdf_retriever,\"pdf_search\",\n",
    "                      \"Search for information about transfomer, GPT-3 and GPT-4 model. For any questions about text generation of languange model, you must use this tool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pdf_search'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_retriever_tool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[wiki,arxiv,retriever_tool,pdf_retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from '/home/kousik/Desktop/Learning_projects/Langchain_tutorial/venv/lib/python3.10/site-packages/wikipedia/__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=200)),\n",
       " ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=200, arxiv_result=<class 'arxiv.Result'>)),\n",
       " Tool(name='langsmith_search', description='Search for information about LangSmith. For any questions about LangSmith, you must use this tool!', args_schema=<class 'langchain_core.tools.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x7f87729003a0>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f8671211f30>), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'), coroutine=functools.partial(<function _aget_relevant_documents at 0x7f8772900550>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f8671211f30>), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n')),\n",
       " Tool(name='pdf_search', description='Search for information about transfomer, GPT-3 and GPT-4 model. For any questions about text generation of languange model, you must use this tool!', args_schema=<class 'langchain_core.tools.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x7f87729003a0>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f8677bdd540>), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'), coroutine=functools.partial(<function _aget_relevant_documents at 0x7f8772900550>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f8677bdd540>), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom dotenv import load_dotenv\\n\\nload_dotenv()\\nimport os\\nos.environ[\\'OPENAI_API_KEY\\']=os.getenv(\"OPENAI_API_KEY\")\\nfrom langchain_openai import ChatOpenAI\\n\\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device_id = 0 if torch.cuda.is_available() else -1\n",
    "#print(device_id)\n",
    "'''\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "# Create the HuggingFacePipeline with GPU support\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    #model_id=\"openai-community/gpt2\",\n",
    "    #model_id=\"mistralai/Mistral-7B-v0.1\",\n",
    "    model_id=\"google/flan-t5-xl\",\n",
    "    task=\"text-generation\",\n",
    "    #device=device_id,\n",
    "    device_map=\"auto\",\n",
    "    pipeline_kwargs={\n",
    "        \"temperature\":0,\n",
    "        \"max_new_tokens\": 150\n",
    "    }\n",
    ")\n",
    "'''\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,\n",
    "         model_name=\"mixtral-8x7b-32768\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprompt = ChatPromptTemplate.from_template(\"\"\"\\nAnswer the following question based only on the provided context. \\nThink step by step before providing a detailed answer. \\nI will tip you $1000 if the user finds the answer helpful. \\n<context>\\n{context}\\n</context>\\n\\nQuestion: {input}\"\"\")\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from langchain import hub\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt.messages\n",
    "'''\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided context. \n",
    "Think step by step before providing a detailed answer. \n",
    "I will tip you $1000 if the user finds the answer helpful. \n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Agents\n",
    "agent=create_openai_tools_agent(llm,tools,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7f866e498730>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7f866e4993c0>, groq_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query']}}}, {'type': 'function', 'function': {'name': 'arxiv', 'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query']}}}, {'type': 'function', 'function': {'name': 'langsmith_search', 'description': 'Search for information about LangSmith. For any questions about LangSmith, you must use this tool!', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query']}}}, {'type': 'function', 'function': {'name': 'pdf_search', 'description': 'Search for information about transfomer, GPT-3 and GPT-4 model. For any questions about text generation of languange model, you must use this tool!', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query']}}}]})\n",
       "| OpenAIToolsAgentOutputParser()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentExecutor(verbose=True, agent=RunnableMultiActionAgent(runnable=RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7f866e498730>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7f866e4993c0>, groq_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query']}}}, {'type': 'function', 'function': {'name': 'arxiv', 'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query']}}}, {'type': 'function', 'function': {'name': 'langsmith_search', 'description': 'Search for information about LangSmith. For any questions about LangSmith, you must use this tool!', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query']}}}, {'type': 'function', 'function': {'name': 'pdf_search', 'description': 'Search for information about transfomer, GPT-3 and GPT-4 model. For any questions about text generation of languange model, you must use this tool!', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query']}}}]})\n",
       "| OpenAIToolsAgentOutputParser(), input_keys_arg=[], return_keys_arg=[], stream_runnable=True), tools=[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from '/home/kousik/Desktop/Learning_projects/Langchain_tutorial/venv/lib/python3.10/site-packages/wikipedia/__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=200)), ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=200, arxiv_result=<class 'arxiv.Result'>)), Tool(name='langsmith_search', description='Search for information about LangSmith. For any questions about LangSmith, you must use this tool!', args_schema=<class 'langchain_core.tools.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x7f87729003a0>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f8671211f30>), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'), coroutine=functools.partial(<function _aget_relevant_documents at 0x7f8772900550>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f8671211f30>), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n')), Tool(name='pdf_search', description='Search for information about transfomer, GPT-3 and GPT-4 model. For any questions about text generation of languange model, you must use this tool!', args_schema=<class 'langchain_core.tools.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x7f87729003a0>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f8677bdd540>), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'), coroutine=functools.partial(<function _aget_relevant_documents at 0x7f8772900550>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f8677bdd540>), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'))])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Agent Executer\n",
    "from langchain.agents import AgentExecutor\n",
    "agent_executor=AgentExecutor(agent=agent,tools=tools,verbose=True)\n",
    "agent_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `langsmith_search` with `{'query': 'Langsmith'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mGet started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith\n",
      "\n",
      "Skip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmith‚ÄãPythonTypeScriptpip install -U langsmithyarn add langchain langsmith2. Create an API key‚ÄãTo create an API key head to the Settings page. Then click Create API Key.3. Set up your environment‚ÄãShellexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>4. Log your first trace‚ÄãWe provide multiple ways to log traces to LangSmith. Below, we'll highlight\n",
      "\n",
      "how to use traceable. See more on the Annotate code for tracing page.PythonTypeScriptimport openaifrom langsmith.wrappers import wrap_openaifrom langsmith import traceable# Auto-trace LLM calls in-contextclient = wrap_openai(openai.Client())@traceable # Auto-trace this functiondef pipeline(user_input: str):    result = client.chat.completions.create(        messages=[{\"role\": \"user\", \"content\": user_input}],        model=\"gpt-3.5-turbo\"    )    return result.choices[0].message.contentpipeline(\"Hello, world!\")# Out:  Hello there! How can I assist you today?import { OpenAI } from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";// Auto-trace LLM calls in-contextconst client = wrapOpenAI(new OpenAI());// Auto-trace this functionconst pipeline = traceable(async (user_input) => {    const result = await client.chat.completions.create({        messages: [{ role: \"user\", content: user_input }],        model: \"gpt-3.5-turbo\",    });\n",
      "\n",
      "\"revision_id\": \"beta\"    },)import { Client, Run, Example } from \"langsmith\";import { evaluate } from \"langsmith/evaluation\";import { EvaluationResult } from \"langsmith/evaluation\";const client = new Client();// Define dataset: these are your test casesconst datasetName = \"Sample Dataset\";const dataset = await client.createDataset(datasetName, {  description: \"A sample dataset in LangSmith.\",});await client.createExamples({  inputs: [    { postfix: \"to LangSmith\" },    { postfix: \"to Evaluations in LangSmith\" },  ],  outputs: [    { output: \"Welcome to LangSmith\" },    { output: \"Welcome to Evaluations in LangSmith\" },  ],  datasetId: dataset.id,});// Define your evaluatorconst exactMatch = async (  run: Run,  example: Example): Promise<EvaluationResult> => {  return {    key: \"exact_match\",    score: run.outputs?.output === example?.outputs?.output,  };};await evaluate(  (input: { postfix: string }) => ({ output: `Welcome ${input.postfix}` }),  {    data: datasetName,    evaluators:\u001b[0m\u001b[32;1m\u001b[1;3mLangSmith is a platform for building production-grade LLM (large language model) applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. You can get started with LangSmith by installing it via pip or npm, creating an API key, and setting up your environment. Once set up, you can log traces using the provided methods and take advantage of LangSmith's features, such as using their wrappers for LLM calls, auto-tracing, and evaluation.\n",
      "\n",
      "For more information on how to use LangSmith and its capabilities, you can refer to the documentation at <https://langsmith.com/docs/>.\n",
      "\n",
      "(Note: I do not have the ability to browse the web, so I cannot directly link you to the LangSmith documentation. However, you can easily find it by searching \"LangSmith documentation\" in your web browser.)\n",
      "\n",
      "Confidence: 90%\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Tell me about Langsmith',\n",
       " 'output': 'LangSmith is a platform for building production-grade LLM (large language model) applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. You can get started with LangSmith by installing it via pip or npm, creating an API key, and setting up your environment. Once set up, you can log traces using the provided methods and take advantage of LangSmith\\'s features, such as using their wrappers for LLM calls, auto-tracing, and evaluation.\\n\\nFor more information on how to use LangSmith and its capabilities, you can refer to the documentation at <https://langsmith.com/docs/>.\\n\\n(Note: I do not have the ability to browse the web, so I cannot directly link you to the LangSmith documentation. However, you can easily find it by searching \"LangSmith documentation\" in your web browser.)\\n\\nConfidence: 90%'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"Tell me about Langsmith\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `arxiv` with `{'query': '1706.03762v7'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPublished: 2023-08-02\n",
      "Title: Attention Is All You Need\n",
      "Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
      "Summary: The do\u001b[0m\u001b[32;1m\u001b[1;3mAttention Is All You Need is a paper published on 2023-08-02, written by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. The paper introduces a new type of neural network architecture called the Transformer, which relies solely on attention mechanisms to perform machine translation tasks. This architecture has been influential in the development of language models like GPT-3 and GPT-4.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"What's the paper 1706.03762v7 about?\",\n",
       " 'output': 'Attention Is All You Need is a paper published on 2023-08-02, written by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. The paper introduces a new type of neural network architecture called the Transformer, which relies solely on attention mechanisms to perform machine translation tasks. This architecture has been influential in the development of language models like GPT-3 and GPT-4.'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"What's the paper 1706.03762v7 about?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conversational_retrieval_chain = ConversationalRetrievalChain.from_llm(\\n    llm=llm,\\n    retriever=retriever,  # Use the previously created retriever\\n    memory=memory,\\n    verbose=True\\n)'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Create an instance of the memory with initial chat history\n",
    "memory = ConversationBufferMemory(memory_key = \"chat_history\", k=5, return_messages=True)\n",
    "\n",
    "# Step 2: Set up the conversational retrieval chain\n",
    "'''conversational_retrieval_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,  # Use the previously created retriever\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent created successfully with conversational retrieval chain\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create an agent that uses the conversational retrieval chain\n",
    "try:\n",
    "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "    print(\"Agent created successfully with conversational retrieval chain\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Failed to create the agent with conversational retrieval chain.\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent executor successfully created with conversational retrieval chain.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create Agent Executor\n",
    "try:\n",
    "    agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory, verbose=True)\n",
    "    print(\"Agent executor successfully created with conversational retrieval chain.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Failed to create the agent executor with conversational retrieval chain.\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `langsmith_search` with `{'query': 'Langsmith'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mGet started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith\n",
      "\n",
      "Skip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startTutorialsHow-to guidesConceptsReferencePricingSelf-hostingLangGraph CloudQuick startOn this pageGet started with LangSmithLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!1. Install LangSmith‚ÄãPythonTypeScriptpip install -U langsmithyarn add langchain langsmith2. Create an API key‚ÄãTo create an API key head to the Settings page. Then click Create API Key.3. Set up your environment‚ÄãShellexport LANGCHAIN_TRACING_V2=trueexport LANGCHAIN_API_KEY=<your-api-key># The below examples use the OpenAI API, though it's not necessary in generalexport OPENAI_API_KEY=<your-openai-api-key>4. Log your first trace‚ÄãWe provide multiple ways to log traces to LangSmith. Below, we'll highlight\n",
      "\n",
      "how to use traceable. See more on the Annotate code for tracing page.PythonTypeScriptimport openaifrom langsmith.wrappers import wrap_openaifrom langsmith import traceable# Auto-trace LLM calls in-contextclient = wrap_openai(openai.Client())@traceable # Auto-trace this functiondef pipeline(user_input: str):    result = client.chat.completions.create(        messages=[{\"role\": \"user\", \"content\": user_input}],        model=\"gpt-3.5-turbo\"    )    return result.choices[0].message.contentpipeline(\"Hello, world!\")# Out:  Hello there! How can I assist you today?import { OpenAI } from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";// Auto-trace LLM calls in-contextconst client = wrapOpenAI(new OpenAI());// Auto-trace this functionconst pipeline = traceable(async (user_input) => {    const result = await client.chat.completions.create({        messages: [{ role: \"user\", content: user_input }],        model: \"gpt-3.5-turbo\",    });\n",
      "\n",
      "\"revision_id\": \"beta\"    },)import { Client, Run, Example } from \"langsmith\";import { evaluate } from \"langsmith/evaluation\";import { EvaluationResult } from \"langsmith/evaluation\";const client = new Client();// Define dataset: these are your test casesconst datasetName = \"Sample Dataset\";const dataset = await client.createDataset(datasetName, {  description: \"A sample dataset in LangSmith.\",});await client.createExamples({  inputs: [    { postfix: \"to LangSmith\" },    { postfix: \"to Evaluations in LangSmith\" },  ],  outputs: [    { output: \"Welcome to LangSmith\" },    { output: \"Welcome to Evaluations in LangSmith\" },  ],  datasetId: dataset.id,});// Define your evaluatorconst exactMatch = async (  run: Run,  example: Example): Promise<EvaluationResult> => {  return {    key: \"exact_match\",    score: run.outputs?.output === example?.outputs?.output,  };};await evaluate(  (input: { postfix: string }) => ({ output: `Welcome ${input.postfix}` }),  {    data: datasetName,    evaluators:\u001b[0m\u001b[32;1m\u001b[1;3mLangSmith is a platform for building production-grade language model applications. You can get started with LangSmith by following these steps:\n",
      "\n",
      "1. Install LangSmith and LangChain using pip or npm.\n",
      "2. Create an API key by going to the Settings page and clicking \"Create API Key\".\n",
      "3. Set up your environment by exporting the necessary variables, such as LANGCHAIN\\_TRACING\\_V2, LANGCHAIN\\_API\\_KEY, and OPENAI\\_API\\_KEY.\n",
      "4. Log your first trace using the provided methods for your programming language.\n",
      "\n",
      "For example, in Python, you can auto-trace LLM calls by wrapping your OpenAI client with the `wrap_openai` function from LangSmith's wrappers.\n",
      "\n",
      "Once you have set up LangSmith, you can start building and evaluating your language model applications using the provided tools and APIs.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Tell me about Langsmith?', 'chat_history': [HumanMessage(content='Tell me about Langsmith?'), AIMessage(content='LangSmith is a platform for building production-grade language model applications. You can get started with LangSmith by following these steps:\\n\\n1. Install LangSmith and LangChain using pip or npm.\\n2. Create an API key by going to the Settings page and clicking \"Create API Key\".\\n3. Set up your environment by exporting the necessary variables, such as LANGCHAIN\\\\_TRACING\\\\_V2, LANGCHAIN\\\\_API\\\\_KEY, and OPENAI\\\\_API\\\\_KEY.\\n4. Log your first trace using the provided methods for your programming language.\\n\\nFor example, in Python, you can auto-trace LLM calls by wrapping your OpenAI client with the `wrap_openai` function from LangSmith\\'s wrappers.\\n\\nOnce you have set up LangSmith, you can start building and evaluating your language model applications using the provided tools and APIs.')], 'output': 'LangSmith is a platform for building production-grade language model applications. You can get started with LangSmith by following these steps:\\n\\n1. Install LangSmith and LangChain using pip or npm.\\n2. Create an API key by going to the Settings page and clicking \"Create API Key\".\\n3. Set up your environment by exporting the necessary variables, such as LANGCHAIN\\\\_TRACING\\\\_V2, LANGCHAIN\\\\_API\\\\_KEY, and OPENAI\\\\_API\\\\_KEY.\\n4. Log your first trace using the provided methods for your programming language.\\n\\nFor example, in Python, you can auto-trace LLM calls by wrapping your OpenAI client with the `wrap_openai` function from LangSmith\\'s wrappers.\\n\\nOnce you have set up LangSmith, you can start building and evaluating your language model applications using the provided tools and APIs.'}\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Invoke the agent with some input\n",
    "try:\n",
    "    response = agent_executor.invoke({\"input\":\"Tell me about Langsmith?\"})\n",
    "\n",
    "    # Print the response\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Failed to invoke the agent.\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe paper with the title \"Attention Is All You Need\" and the ID 1706.03762v7 is a groundbreaking work in the field of natural language processing (NLP). It was published by Ashish Vaswani and his colleagues at Google Brain in 2017.\n",
      "\n",
      "In this paper, the authors introduce the Transformer model, a novel architecture for NLP tasks that eliminates the need for recurrence and convolutions. Instead, it relies entirely on self-attention mechanisms to process input sequences.\n",
      "\n",
      "The Transformer model has several benefits compared to traditional architectures like LSTMs and GRUs. For example, it is parallelizable and can process input sequences of arbitrary lengths. This makes it well-suited for tasks like machine translation and text summarization, where input and output sequences can be very long.\n",
      "\n",
      "The paper presents the Transformer model in detail, including its architecture and training procedure. It also includes experiments that demonstrate the model's superior performance on several NLP tasks, such as machine translation and text classification.\n",
      "\n",
      "The Transformer model has since become a standard architecture for NLP tasks and has inspired many subsequent works in the field. \n",
      "\n",
      "To summarize, the paper 1706.03762v7 is about the introduction of the Transformer model, a novel architecture for NLP tasks that relies entirely on self-attention mechanisms, eliminating the need for recurrence and convolutions. The Transformer model has several benefits, such as being parallelizable and able to process input sequences of arbitrary lengths. The paper includes experiments demonstrating the model's superior performance on several NLP tasks, such as machine translation and text classification.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': \"What's the paper 1706.03762v7 about?\", 'chat_history': [HumanMessage(content='Tell me about Langsmith?'), AIMessage(content='LangSmith is a platform for building production-grade language model applications. You can get started with LangSmith by following these steps:\\n\\n1. Install LangSmith and LangChain using pip or npm.\\n2. Create an API key by going to the Settings page and clicking \"Create API Key\".\\n3. Set up your environment by exporting the necessary variables, such as LANGCHAIN\\\\_TRACING\\\\_V2, LANGCHAIN\\\\_API\\\\_KEY, and OPENAI\\\\_API\\\\_KEY.\\n4. Log your first trace using the provided methods for your programming language.\\n\\nFor example, in Python, you can auto-trace LLM calls by wrapping your OpenAI client with the `wrap_openai` function from LangSmith\\'s wrappers.\\n\\nOnce you have set up LangSmith, you can start building and evaluating your language model applications using the provided tools and APIs.'), HumanMessage(content=\"What's the paper 1706.03762v7 about?\"), AIMessage(content='The paper with the title \"Attention Is All You Need\" and the ID 1706.03762v7 is a groundbreaking work in the field of natural language processing (NLP). It was published by Ashish Vaswani and his colleagues at Google Brain in 2017.\\n\\nIn this paper, the authors introduce the Transformer model, a novel architecture for NLP tasks that eliminates the need for recurrence and convolutions. Instead, it relies entirely on self-attention mechanisms to process input sequences.\\n\\nThe Transformer model has several benefits compared to traditional architectures like LSTMs and GRUs. For example, it is parallelizable and can process input sequences of arbitrary lengths. This makes it well-suited for tasks like machine translation and text summarization, where input and output sequences can be very long.\\n\\nThe paper presents the Transformer model in detail, including its architecture and training procedure. It also includes experiments that demonstrate the model\\'s superior performance on several NLP tasks, such as machine translation and text classification.\\n\\nThe Transformer model has since become a standard architecture for NLP tasks and has inspired many subsequent works in the field.'), HumanMessage(content=\"What's the paper 1706.03762v7 about?\"), AIMessage(content='The paper with the title \"Attention Is All You Need\" and the ID 1706.03762v7 is a groundbreaking work in the field of natural language processing (NLP). It was published by Ashish Vaswani and his colleagues at Google Brain in 2017.\\n\\nIn this paper, the authors introduce the Transformer model, a novel architecture for NLP tasks that eliminates the need for recurrence and convolutions. Instead, it relies entirely on self-attention mechanisms to process input sequences.\\n\\nThe Transformer model has several benefits compared to traditional architectures like LSTMs and GRUs. For example, it is parallelizable and can process input sequences of arbitrary lengths. This makes it well-suited for tasks like machine translation and text summarization, where input and output sequences can be very long.\\n\\nThe paper presents the Transformer model in detail, including its architecture and training procedure. It also includes experiments that demonstrate the model\\'s superior performance on several NLP tasks, such as machine translation and text classification.\\n\\nThe Transformer model has since become a standard architecture for NLP tasks and has inspired many subsequent works in the field. \\n\\nTo summarize, the paper 1706.03762v7 is about the introduction of the Transformer model, a novel architecture for NLP tasks that relies entirely on self-attention mechanisms, eliminating the need for recurrence and convolutions. The Transformer model has several benefits, such as being parallelizable and able to process input sequences of arbitrary lengths. The paper includes experiments demonstrating the model\\'s superior performance on several NLP tasks, such as machine translation and text classification.')], 'output': 'The paper with the title \"Attention Is All You Need\" and the ID 1706.03762v7 is a groundbreaking work in the field of natural language processing (NLP). It was published by Ashish Vaswani and his colleagues at Google Brain in 2017.\\n\\nIn this paper, the authors introduce the Transformer model, a novel architecture for NLP tasks that eliminates the need for recurrence and convolutions. Instead, it relies entirely on self-attention mechanisms to process input sequences.\\n\\nThe Transformer model has several benefits compared to traditional architectures like LSTMs and GRUs. For example, it is parallelizable and can process input sequences of arbitrary lengths. This makes it well-suited for tasks like machine translation and text summarization, where input and output sequences can be very long.\\n\\nThe paper presents the Transformer model in detail, including its architecture and training procedure. It also includes experiments that demonstrate the model\\'s superior performance on several NLP tasks, such as machine translation and text classification.\\n\\nThe Transformer model has since become a standard architecture for NLP tasks and has inspired many subsequent works in the field. \\n\\nTo summarize, the paper 1706.03762v7 is about the introduction of the Transformer model, a novel architecture for NLP tasks that relies entirely on self-attention mechanisms, eliminating the need for recurrence and convolutions. The Transformer model has several benefits, such as being parallelizable and able to process input sequences of arbitrary lengths. The paper includes experiments demonstrating the model\\'s superior performance on several NLP tasks, such as machine translation and text classification.'}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = agent_executor.invoke({\"input\":\"What's the paper 1706.03762v7 about?\"})\n",
    "# Print the response\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Failed to invoke the agent.\") from e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
